import argparse
import json
import requests

def getParams():
    '''Parse parameters from the command line'''
    parser = argparse.ArgumentParser(description = """
============
Retrieve ENCODE metadata of ENCSR experiments from a search url generated by using the ENCODE portal.
Example URLs:
    - All conservative released ENCFF peak files for "TF ChIP-seq" of four cell lines: MCF-7, K562, A549, HepG2
https://www.encodeproject.org/search/?type=File&file_format_type=narrowPeak&file_type=bed+narrowPeak&assay_title=TF+ChIP-seq&biosample_ontology.term_name=K562&biosample_ontology.term_name=A549&biosample_ontology.term_name=MCF-7&biosample_ontology.term_name=HepG2&assembly=GRCh38&status=released&output_type=conservative+IDR+thresholded+peaks&format=json&limit=all

Use the left bar search filters to narrow down by assay, biosample, etc.

Remember to use quotation marks around url!
============
""", formatter_class = argparse.RawTextHelpFormatter)

    parser.add_argument('-i','--input', metavar='search_url', required=True, help='the search url string. Be sure to check for "Experiment" type.')
    parser.add_argument('-o','--output', metavar='tsv_fn', required=True, help='the output tab-delimited accessions and relevant metadata')

    args = parser.parse_args()
    return(args)


# Helper: ENCFF to URL to payload
def fetch_data(url):
    # Force return from the server in JSON format
    headers = {'accept': 'application/json'}

    # GET the search result
    response = requests.get(url, headers=headers)

    # Extract the JSON response as a python dictionary
    search_results = response.json()
    return(search_results)


# Main program which takes in input parameters
if __name__ == '__main__':

    # Get params
    args = getParams()

    # Initialize storage variable
    target2info = {}

    # Get payload for search
    payload = fetch_data(args.input)
    payload = payload.get("@graph", None)
    if (payload == None):
        print("Error: Returned JSON without \"@graph\"")
        quit()
    # print(json.dumps(data,indent=4))

    # Initialize line_storage
    lines = []

    # Parse payload for each accession extract data from each hit
    for data in payload:
        # Save File accession
        accession = data.get('accession', 'ENCFFXXXXXX').strip()

        # Get File accession
        dataset = data.get('dataset', None)

        schema_version = data.get('schema_version',"No Schema Version")
        print("====== %s ======" % accession)
        print("v%s" % schema_version)

        # Get Biosample info
        biosample_name = data.get('biosample_ontology', None).get('term_name', None)

        # Get Assay info
        assay_term_name = data.get('assay_term_name', None)

        # Get Assembly
        assembly = data.get('assembly', None)

        # Get Target
        target_name = data.get('target', None).get('label', None)

        # Get Output type
        output_type = data.get('output_type', None)

        # Get File Format info
        file_format = data.get('file_format', None)
        file_format_type = data.get('file_format_type', None)

        # Get Derived from file list
        derived_from = '|'.join([ i for i in data.get('derived_from', None)])

        # Future additions?
        # - add audit information
        # - add quality metrics
        # - add replicate info

        # Update lines with new metadata
        row = [accession, dataset, biosample_name, target_name, assay_term_name, assembly, output_type, file_format, file_format_type, derived_from]

        lines.append('\t'.join([str(i) for i in row]))

    headers = ["accession", "dataset", "biosample_name", "target_name", "assay_term_name", "assembly", "output_type", "file_format", "file_format_type", "derived_from"]
    # Writing to sample.json
    with open(args.output, "w") as outfile:
        outfile.write('\t'.join(headers) + "\n")
        outfile.write('\n'.join(lines))

    # Write metadata with download URL
    # [ sys.stdout.write('\t'.join(info) + "\n") for info in target2info.values() ]
